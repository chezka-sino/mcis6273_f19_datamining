{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chezka Sino (9028-67538)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCIS6273 Data Mining (Prof. Maull) / Fall 2019 / HW2\n",
    "\n",
    "**This assignment is worth up to 40 POINTS to your grade total if you complete it on time.**\n",
    "\n",
    "| Points <br/>Possible | Due Date | Time Commitment <br/>(estimated) |\n",
    "|:---------------:|:--------:|:---------------:|\n",
    "| 40 | Wednesday, December 11 @ Midnight | _up to_ 12 hours |\n",
    "\n",
    "\n",
    "* **GRADING:** Grading will be aligned with the completeness of the objectives.\n",
    "\n",
    "* **INDEPENDENT WORK:** Copying, cheating, plagiarism  and academic dishonesty _are not tolerated_ by University or course policy.  Please see the syllabus for the full departmental and University statement on the academic code of honor.\n",
    "\n",
    "## OBJECTIVES\n",
    "* improve on your homework 1 assignment, if necessary\n",
    "\n",
    "* continue exploratory data analysis and visualization\n",
    "\n",
    "* perform a clustering analysis using k-means\n",
    "\n",
    "## WHAT TO TURN IN\n",
    "You are being encouraged to turn the assignment in using the provided\n",
    "Jupyter Notebook.  To do so, make a directory in your Lab environment called\n",
    "`homework/hw2`.   Put all of your files in that directory.  Then zip that directory,\n",
    "rename it with your name as the first part of the filename (e.g. `maull_hw2_files.zip`), then\n",
    "download it to your local machine, then upload the `.zip` to Blackboard.\n",
    "\n",
    "If you do not know how to do this, please ask, or visit one of the many tutorials out there\n",
    "on the basics of using zip in Linux.\n",
    "\n",
    "## ASSIGNMENT TASKS\n",
    "### (25%) continue exploratory data analysis and visualization \n",
    "\n",
    "In the last HW we explore some of the bas features of Pandas with graphic and data selection.\n",
    "This time we're going to go a but deeper into Pandas ans learn about MultIndices and grouping\n",
    "data in interesting and useful ways.\n",
    "\n",
    "One of the things that we learned from the data last time is that the majority of it are\n",
    "interesting over several dimensions.  There are the years of competition, the sex of the competitors,\n",
    "the age the competitors, country of origin, among other things.  With denser data like these, we want\n",
    "to understand some of the underlying groupings for easier access to the data.  For example,\n",
    "one might want to understand how groupings by year and age bear out on the data to explore questions\n",
    "like \"Has the number of competitors over 40 increased over the years?\"  This might be an interesting\n",
    "question to ask to explore if powerlifters continue to compete as they age since the sport is very\n",
    "difficult on one's body and requires intense continuous training to stay competitive.\n",
    "\n",
    "Some questions like these are also very useful to explore visually, so we'll dive into a few more\n",
    "graphical techniques to get at these answers and more.  We're going to end up with a DataFrame\n",
    "that will group our data by year, age class and sex, so we can see some of the interesting\n",
    "annual trends along each of these dimensions within the last two decades.\n",
    "\n",
    "&#167;  Let's first get a feel for the data and filter it down.  One of the main\n",
    "difficulties in dealing with large user-contributed data sets like these\n",
    "are _data consistency_ and _data quality_.  _Data consistency_ refers to\n",
    "how data is represented over time.  We can see how this becomes an issue\n",
    "when we look at the `Division` column of the dataset.  We can see with a\n",
    "relatively untrained eye to the data, that something is very wrong with\n",
    "the consistency &mdash; there are over 1300 Division designations!  When\n",
    "you look at it more closely, there are groupings that overlap.  For\n",
    "example, you will see `Masters 45-49` and `Masters 40-49` when you\n",
    "perform a `.value_counts()` on the `Divisions` column of the data (see\n",
    "supplemental notebook).  What is the difference between these two since\n",
    "they obviously overlap?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the data down to a smaller subset of the data using\n",
    "[`dropna()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html).\n",
    "Specifically, drop all rows of data missing `Age`\n",
    "and `Division` data (e.g. they have `NaN` values).  You will be able to do this by passing a\n",
    "`subset=` parameter into `dropna()`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now have fewer than 200K data points, leading us to the second\n",
    "problem of _data quality_.  When we look at the original data file, we have\n",
    "over 400K data points, yet after filtering for missing values, we end up\n",
    "with over 50% reduction in the data!  This is actually a reasonable preservation\n",
    "of the original data set given that we `NaN` values from two different columns.\n",
    "In general, it isn't a bad idea when working with your data to develop an\n",
    "understanding of the holes in it.  What if we reduced our data 90% just on\n",
    "`Age` indicating it was not recorded consistently over time?  This would indeed\n",
    "limit what we could ask of the data in analyses requiring age data.  Many of\n",
    "the tools in Pandas will ignore `NaN` data, but some required you to send\n",
    "specific instructions to the tool on what to do.  Better to get ahead of things\n",
    "now.\n",
    "\n",
    "In your notebook **you must include the following** to be considered a correct answer:\n",
    "\n",
    "* correct use of `dropna()`,\n",
    "* output of the original dimensions of the data using `DataFrame.shape` or\n",
    "  something similar that shows the original and new dimensions of the data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#167;  One of the major issues with any data set when importing into Pandas is the\n",
    "that Pandas tries to infer the data types so that when you compare data\n",
    "you are comparing data that _can_ be compared (i.e. you cannot  compare\n",
    "strings with integers).  One area that you'll need to be mindful\n",
    "of is with dates.  For example, if you perform a `df.Date.head()` the\n",
    "data is of type `object`.  We want it to be `Datetime` so we can benefit\n",
    "from the many useful features in Pandas to manipulate dates.\n",
    "\n",
    "Doing this is straightforward with the [`pandas.datetime()`](http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_datetime.html#pandas.to_datetime) method.\n",
    "Your notebook must show:\n",
    "\n",
    "* use of `pandas.datetime` to convert the `Date` column\n",
    "  of your data to `datetime` (hint: use `loc` to set the data),\n",
    "* that the column has changed by using `DataFrame.dtype` or alternatively\n",
    "  use `DataFrame.head()` which will show the data type in its output.\n",
    "\n",
    "**NOTE**: you may need to use the `errors` parameter of `to_datetime` to\n",
    "handle any issues you may encounter.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#167;  Pandas provides superior capabilities to slice and group data.  We would like\n",
    "to build a dataset that is composed of all remaining data from\n",
    "1999 to 2018 that restricts age to those 21 and older.\n",
    "\n",
    "Like the last homework, doing this is simple with [`Dataframe.query()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.query.html#pandas.DataFrame.query).  You will notice that a\n",
    "query over the `Date` column can be done naturally by just saying\n",
    "`Date > a_year` since it is a `datetime` object.  Pandas does the inferential\n",
    "magic for you!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next add to your query by grouping the data by `Date`, `AgeClass`\n",
    "and `Sex`.   You will need to use the [`DataFrame.groupby()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html#pandas.DataFrame.groupby)\n",
    "method to accomplish this.  **A hint for grouping by year**: if you do not restrict\n",
    "the data in the `Date` column Pandas will naturally group by the\n",
    "full date meaning that each _day_ will be grouped leading to the\n",
    "wrong result.  You can convert a date to a year for the purposes of\n",
    "grouping by year using `dt.to_period('Y')`.  Please see the documentation for\n",
    "[`Series.dt.to_period()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.dt.to_period.html).\n",
    "\n",
    "You will pass to the first parameter of `DataFrame.groupby()` the list of\n",
    "the grouping in order of grouping, outer group first.  Thus, `groupby(['Sex', 'AgeClass`])\n",
    "will return the MultiIndex DataFrame with `Sex` as the outermost group\n",
    "and `AgeClass` the inner group.  See the supplemental notebook for some\n",
    "more clues.\n",
    "\n",
    "Your notebook must show:\n",
    "\n",
    "* use of `query()` to restrict your data to competitors 21 and older\n",
    "  in competitions from 1999 to 2018,\n",
    "* use of `groupby` showing the **`mean()`** values for each column with\n",
    "  groups being (in order) `Date (year)`, `AgeClass` and `Sex`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#167;  Now that we have the data segmented the way we'd like, let's visualize it\n",
    "in some interesting way.\n",
    "\n",
    "With powerlifting there are a number of ways to express the _strength_ of a\n",
    "competitor. There is _raw_ strength, meaning how much total weight was lifted\n",
    "on a given lift, and there is _relative_ strength.  It is not fair to compare\n",
    "the raw lift of a 100lb 16 year old teenage to that of a 35 year old 300lb adult.\n",
    "\n",
    "The 35 year old might lift ten times the weight yet the 16 year old may be _relatively_\n",
    "stronger, but how would we compare their\n",
    "_relative_ strenghts?  Many competitors will be able to lift between 2 and 7 times\n",
    "their body weight depending on the lift, so we might expect a 100lb powerlifter\n",
    "to perhaps perform a 200lb bench press and maybe a 300lb squat, both impressive\n",
    "for their weight.  To deal with comparing _strength_ across age and weight variables\n",
    "a number of methods have been developed to create fair and accurate measures of\n",
    "_relative strength_.  The OpenPowelifting dataset includes three such measures:\n",
    "_Wilks_, _McCulloch_ and _Glossbrenner_, which give a numeric assignment of relative\n",
    "strength which factor age and weight into the computation.  Exploring the details\n",
    "of each of these methods is beyond the scope of this homework, but the curious can\n",
    "learn more on the variety of sites which calculate these statistics.\n",
    "\n",
    "We will restrict our interest to the _Glossbrenner_ score, which takes into account\n",
    "age and weight to compute a normalized weight value.  Consider three competitors,\n",
    "all 29 year olds, with one male and one female weighting 131.84lbs and the last male\n",
    "weighting 263.67 pounds.  Assume they all lift 639.33 pounds total.  The _Glossbrenner_ score\n",
    "takes into account the age and weights and produces a relative score with the\n",
    "following:\n",
    "\n",
    "| competitor | sex | age (lbs) | weight (lbs) | lift (lbs) | score | $\\gamma$-coefficient | $\\gamma_{age}$-coefficient\n",
    "|-----------:|:---:|:---:|:------:|:-----:|:-----:|:--------------------:|:----:|\n",
    "| 1 | F | 29 |  131.84 | 639.33 | 287.187 | 0.9903 | 1 |\n",
    "| 2 | M | 29 |  131.84 | 639.33 | 242.3095 | 0.83555 | 1 |\n",
    "| 3 | M | 29 |  263.67 | 639.33 | 159.906 | 0.5514 | 1 |\n",
    "| 4 | M | 49 | 263.67 | 639.33 | 218.951 | 0.5514 | 1.113 |\n",
    "| 5 | F | 49 | 263.67 | 639.33 | 177.9754 | 0.67835 | 1.113 |\n",
    "\n",
    "\n",
    "The _Glossbrenner_ score is in the _score_ column and the $\\gamma$-coefficient is\n",
    "the constant calculated by the method.  The $\\gamma_{age}$-coefficient is computed\n",
    "constant which the method factors in for the relative impact age has on the\n",
    "competitor.  Thus, the _Glossbrenner_ score $\\Gamma$ is:\n",
    "\n",
    "$$\n",
    "\\Gamma(age, sex) = \\gamma{\\text{-coefficient}}_{age,sex} \\times \\gamma_{age} \\times weight\n",
    "$$\n",
    "\n",
    "If you perform the score calculation on the Openpowerlifting data, you'll notice the\n",
    "some values are off by a small amount and this is likely due to the _Glossbrenner_ constants\n",
    "used, which have varied over time.\n",
    "\n",
    "Now that we have that out of the way, let's visualize some data.  Specifically, we'd like to\n",
    "plot the _Glossbrenner_ score for the last 20 years over time.  Are the scores going up, down\n",
    "or staying the same?  One could expect any of these scenarios to occur, so let's dive in.\n",
    "\n",
    "What we want to produce are two _area_ plots of the annual _mean Glossbrenner_ score\n",
    "from 1999 to 2018 for all age groups, one plot for males and the other for females as putting them all on\n",
    "one graph would most certainly be information overload.  To do this we will need to\n",
    "slice the data in a way that makes a multi-index grouped by year, age group and sex.\n",
    "You will effectively use the data from the prior part and make a visual of it.\n",
    "\n",
    "Your area plot code will be invoked by:\n",
    "\n",
    "```python\n",
    "\n",
    "DataFrame.plot.area()\n",
    "\n",
    "```\n",
    "\n",
    "You may optionally pass in the `figsize=(15,7)` (or whatever dimensions you'd like)\n",
    "to stretch the data out a bit so you can visually see what is going on, since the\n",
    "legend may get in the way of viewing the data.\n",
    "\n",
    "Your plot will look something like this:\n",
    "\n",
    "![](./sample_area.png)\n",
    "\n",
    "Please see the [`DataFrame.plot.area()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.area.html#pandas.DataFrame.plot.area) method for full information on\n",
    "the area plots.\n",
    "\n",
    "**A final important note**: You will need to use [`droplevel()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.droplevel.html) and\n",
    "[`unstack()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.unstack.html)\n",
    "in order to prepare your DataFrame for final presentation.  Basically,\n",
    "you'll need to drop the `Sex` level of your index (level 2) and immediately\n",
    "before you plot the area plot you will use `unstack()`.\n",
    "\n",
    "Your notebook must show:\n",
    "\n",
    "* an area plot showing the male data grouped by age and year, that\n",
    "  is the $x$-axis will show the year and the $y$-axis the _Glossbrenner_ score,\n",
    "* an area plot showing the female data grouped by age and year, that\n",
    "  is the $x$-axis will show the year and the $y$-axis the _Glossbrenner_ score.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You must also answer the following questions in your notebook:\n",
    "\n",
    "* What's the general trend you see in the area plots?  Your answer can\n",
    "  be in one or two sentences.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (50%) perform a clustering analysis using k-means \n",
    "\n",
    "The simplicity and power of k-means algorithm makes it one of the best to start with\n",
    "when performing _unsupervised learning_ &mdash; that is the class labels of your\n",
    "data are not known _a priori_ and you that will not be training the algorithm\n",
    "on labeled data.  While this is a powerful and oft useful technique, use it with\n",
    "care as the initial conditions of the algorithm do not guarantee a global maximum\n",
    "and as such, running the algorithm with a number of initialization points will\n",
    "produce better and more reliable results.\n",
    "\n",
    "Continuing with our OpenPowerlifting data, we're going to do some exploratory data\n",
    "analysis to examine this dataset in some interesting ways using unsupervised learning,\n",
    "namely clustering.  The original dataset has over 1 million data points, but in order\n",
    "to get a good idea of what's in it, we will not need to go back through the entire\n",
    "dataset, and in fact, we will restrict the focus of our energy on just the last 2 decades\n",
    "from 1999.\n",
    "\n",
    "**REMEMBER TO MAKE SURE TO SHOW ALL YOUR WORK IN THE NOTEBOOK SO YOU CAN RECEIVE PARTIAL CREDIT WHERE APPROPRIATE!**\n",
    "\n",
    "&#167;  You will need to complete part 1 of this homework to filter the data to the necessary\n",
    "subset for this part.  As we talked about in lecture, the subset of features will just\n",
    "be the following:\n",
    "\n",
    "  ```python\n",
    "  features = [\n",
    "       'Sex',\n",
    "       'Age',\n",
    "       'BodyweightKg',\n",
    "       'Best3SquatKg',\n",
    "       'Best3BenchKg',\n",
    "       'Best3DeadliftKg',\n",
    "       'TotalKg',\n",
    "       'Date'\n",
    "  ]\n",
    "  ```\n",
    "\n",
    "Final preparation for clustering will require you to turn all of\n",
    "_categorical_ variables into _numeric_ one's.  One way to do this\n",
    "from directly within Pandas is to use `Pandas.get_dummies(your_dataframe)`.\n",
    "You can also study the\n",
    "`sklearn.preprocessing.OrdinalEncoder()` which will do something\n",
    "very similar.  Either way, with the reduced set of features above, the only\n",
    "categorical variable will be `Sex` as all the others should already\n",
    "be numerical features.\n",
    "\n",
    "In your notebook, you should show:\n",
    "\n",
    "* clearly how many features are now in your dataframe?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#167;  In class we talked about the fact that the $k$ number of clusters needs to be\n",
    "determined _a priori_ &mdash; that is you will need to know how many clusters beforehand to\n",
    "run the algorithm.  To find the optimal $k$, we will use a method called the _silhouette score_.\n",
    "\n",
    "Adapt the following code to compute the silhouette scores on *only* the dataset filtered by\n",
    "the features from the prior step.\n",
    "\n",
    "```python\n",
    "  from sklearn.cluster import KMeans\n",
    "  from sklearn.metrics import silhouette_score\n",
    "\n",
    "  Sum_of_squared_distances = []\n",
    "  K = range(2, 15)\n",
    "  for k in K:\n",
    "      km = KMeans(n_clusters=k, n_init=20)\n",
    "      km = km.fit(YOUR_OPENPOWERLIFTING_DATAFRAME_WITH_DUMMY_VARS)\n",
    "      Sum_of_squared_distances.append(km.inertia_)\n",
    "\n",
    "      silh_score = silhouette_score(YOUR_OPENPOWERLIFTING_DATAFRAME_WITH_DUMMY_VARS, km.labels_)\n",
    "      print(\"k = {} | silhouette_score = {}\".format(k, silh_score))\n",
    "```\n",
    "\n",
    "  The largest score is typically the $k$ you go with.  If $k=2$ is your largest\n",
    "  score, we will ignore and use the next best score since 2 clusters is not usually an\n",
    "  interesting number of clusters when dealing with a large set of data points.\n",
    "\n",
    "  Your notebook must show and answer the following:\n",
    "\n",
    "  * What is the optimal $k$ according the silhouette score?\n",
    "  * What else is interesting about the scores?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#167;  Now that you have clusters and optimal cluster, let's find out the characteristics of\n",
    "the features that dominate them.\n",
    "\n",
    "Note that the k-means algorithm returns the cluster centers\n",
    "for each cluster, hence in that center each feature value\n",
    "is the _representative feature value_ for that cluster.\n",
    "For example, the `TotalKg` would be the representative `TotalKg` for\n",
    "that cluster.\n",
    "\n",
    "Using the optimal cluster size from the silhouette score in the prior\n",
    "section, please use adapt the following code to determine the cluster\n",
    "characteristics.\n",
    "\n",
    "```python\n",
    "    optimal_k = THE_OPTIMAL_SILH_K\n",
    "\n",
    "    km = KMeans(n_clusters=optimal_k, n_init=150)\n",
    "    km = km.fit(YOUR_OPENPOWERLIFTING_DATAFRAME_WITH_DUMMY_VARS)\n",
    "\n",
    "    for i in range(0, optimal_k):\n",
    "        l = list(zip(YOUR_OPENPOWERLIFTING_DATAFRAME_WITH_DUMMY_VARS.columns, \\\n",
    "                    km.cluster_centers_[i]))\n",
    "        l.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        print('CLUSTER : {}\\n'.format(i))\n",
    "        for attr, val in l[:]:\n",
    "          print('\\t{} : {}\\n'.format(attr, val))\n",
    "```\n",
    "\n",
    "Your notebook must show and answer the following:\n",
    "\n",
    "* for each cluster, describe in real words what the cluster centers are telling\n",
    "  you about the representative of that cluster.  For example, your answer might\n",
    "  look like: \"for cluster 1, the representative for that cluster is a 24.7 year\n",
    "  old female, with an average `Best3SquatKg` of 121 and a `TotalKg` of 721\",\n",
    "* show the output of the cluster centers above.\n",
    "\n",
    "**NOTE**: The order of the features in `km.cluster_centers_` are the same order\n",
    "as they exist in the DataFrame.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": "1",
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
