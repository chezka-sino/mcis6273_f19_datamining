{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCIS6273 Data Mining (Prof. Maull) / Fall 2019 / HW0\n",
    "\n",
    "**This assignment is worth up to 20 POINTS to your grade total if you complete it on time.**\n",
    "\n",
    "| Points <br/>Possible | Due Date | Time Commitment <br/>(estimated) |\n",
    "|:---------------:|:--------:|:---------------:|\n",
    "| 20 | Wednesday, Sep 11 @ Midnight | _up to_ 20 hours |\n",
    "\n",
    "\n",
    "* **GRADING:** Grading will be aligned with the completeness of the objectives.\n",
    "\n",
    "* **INDEPENDENT WORK:** Copying, cheating, plagiarism  and academic dishonesty _are not tolerated_ by Univerisity or course policy.  Please see the syllabus for the full departmental and University statement on the academic code of honor.\n",
    "\n",
    "## OBJECTIVES\n",
    "* Familiarize yourself with the JupyterLab environment, Markdown and Python\n",
    "\n",
    "* Familiarize yourself with Github and basic git\n",
    "\n",
    "* Explore JupyterHub Linux console integrating what you learned in the prior parts of this homework\n",
    "\n",
    "* Listen to the O'Reilly Data Show Podcast from July 18, 2019: Acquiring and shairing high-quality data with Rogen Chen\n",
    "\n",
    "* Explore Python for basic text mining\n",
    "\n",
    "* Explore Python for data munging and analysis, with an introduction to JSON and Pandas\n",
    "\n",
    "## WHAT TO TURN IN\n",
    "You are being encouraged to turn the assignment in using the provided\n",
    "Jupyter Notebook.  To do so, make a directory in your Lab environment called\n",
    "`homework/hw0`.   Put all of your files in that directory.  Then zip that directory,\n",
    "rename it with your name as the first part of the filename (e.g. `maull_hw0_files.zip`), then\n",
    "download it to your local machine, then upload the `.zip` to Blackboard.\n",
    "\n",
    "If you do not know how to do this, please ask, or visit one of the many tutorials out there\n",
    "on the basics of using zip in Linux.\n",
    "\n",
    "If you choose not to use the provided notebook, you will still need to turn in a\n",
    "`.ipynb` Jupyter Notebook and corresponding files according to the instructions in\n",
    "this homework.\n",
    "\n",
    "## ASSIGNMENT TASKS\n",
    "### (0%) Familiarize yourself with the JupyterLab environment, Markdown and Python \n",
    "\n",
    "As stated in the course announcement [Jupyter (https://jupyter.org)](https://jupyter.org) is the\n",
    "core platform we will be using in this course and\n",
    "is a popular platform for data scientists around the world.  We have a JupyterLab\n",
    "setup for this course so that we can operate in a cloud-hosted environment, free from\n",
    "some of the resource contraints of running Jupyter on your local machine (though you are free to set\n",
    "it up on your own and seek my advice if you desire).\n",
    "\n",
    "You have been given the information about the  Jupyter envitonment we have setup for our course, and\n",
    "the underlying Python environment will be using is the [Anaconda (https://anaconda.com)](https://anaconda.com)\n",
    "distribution.  It is not necessary for this assignment, but you are free to look at the multitude\n",
    "of packages installed with Anaconda, though we will not use the majority of them explicitly.\n",
    "\n",
    "As you will soon find out, Notebooks are an incredibly effective way to mix code with narrative\n",
    "and you can create cells that are entirely code or entirely Markdown.  Markdown (MD or md) is\n",
    "a highly readable text format that allows for easy documentation of text files, while allowing\n",
    "for HTML-based rendering of the text in a way that is style-independent.\n",
    "\n",
    "We will be using Markdown frequently in this course, and you will learn that there are many different\n",
    "\"flavors\" or Markdown.  We will only be using the basic flavor, but you will benefit from exploring\n",
    "the \"Github flavored\" Markdown, though you will not be responsible for using it in this course -- only the\n",
    "\"basic\" flavor.  Please refer to the original course announcement about Markdown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#167;  **THERE IS NOTHING TO TURN IN FOR THIS PART.** Play with and become familiar with the basic functions of\n",
    "the Lab environment given to you online in the course Blackboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#167;  **PLEASE _CREATE A MARKDOWN DOCUMENT_ CALLED `semester_goals.md` WITH 3 SENTENCES/FRAGMENTS THAT\n",
    "ANSWER THE FOLLOWING QUESTION:**\n",
    "\n",
    "* **What do you wish to accomplish this semester in Data Mining?**\n",
    "\n",
    "Read the documentation for basic Markdown [here](https://www.markdownguide.org/basic-syntax). \n",
    "Turn in the text `.md` file *not* the processed `.html`.  In whatever you turn in, \n",
    "you must show the use of *ALL* the following:\n",
    "\n",
    "* headings (one level is fine),\n",
    "* bullets,\n",
    "* bold and italics\n",
    "\n",
    "Again, the content of your document needs to address the question above and it should live\n",
    "in the top level directory of your assignment submission.  This part will be graded but no\n",
    "points are awarded for your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (0%) Familiarize yourself with Github and basic git \n",
    "\n",
    "[Github (https://github.com)](https://github.com) is the _de facto_ platform for open source software in the world based\n",
    "on the very popular [git (https://git-scm.org)](https://git-scm.org) version control system. Git has a sophisticated set\n",
    "of tools for version control based on the concept of local repositories for fast commits and remote\n",
    "repositories only when collaboration and remote synchronization is necessary.  Github enhances git by providing\n",
    "tools and online hosting of public and private repositories to encourage and promote sharing and collaboration.\n",
    "Github hosts some of the world's most widely used open source software.\n",
    "\n",
    "**If you are already familiar with git and Github, then this part will be very easy!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#167;  **CREATE A GITHUB.COM ACCOUNT AND RECORD THE URL TO YOUR ACCOUNT.**  It is OK if you already have an account, \n",
    "record the account URL either way. Use your `.edu` email address -- you will be able to get free private repositories\n",
    "for as long as you are a student with that email address.  Enjoy this perk while you can -- private repositories\n",
    "start at around $7/month when you are no longer using the `.edu` address.  Public repositories are always free."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#167;  **CREATE A PUBLIC GITHUB REPO NAME `\"MyDataSci2019\"` AND PLACE A README.MD FILE IN IT.**\n",
    "Create your first file called\n",
    "`README.md` at the top level of the repository.  You can put whatever text you like in the file \n",
    "(If you like, use something like [lorem ipsum](https://lipsum.com/)\n",
    "to generate random sentences to place in the file.).\n",
    "Please include the link to **your** Github repository that now includes the minimal `README.md`. \n",
    "You don't have to have anything elaborate in that file or the repo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (0%) Explore JupyterHub Linux console integrating what you learned in the prior parts of this homework \n",
    "\n",
    "The Linux console in JupyterLab is a great way to perform command-line tasks and is an essential tool\n",
    "for basic scripting that is part of a data scientist's toolkit.  Open a console in the lab environment\n",
    "and familiarize yourself with your files and basic commands using git as indicated below.\n",
    "\n",
    "1. In a new JupyterLab command line console, run the `git clone` command to clone the new\n",
    "  repository you created in the prior part.\n",
    "  You will want to read the documentation on this \n",
    "  command (try here [https://www.git-scm.com/docs/git-clone](https://www.git-scm.com/docs/git-clone) to get a good\n",
    "  start).\n",
    "2. Within the same console, modify your `README.md` file, check it in and push it back to your repository, using\n",
    "  `git push`.  Read the [documentation about `git push`](https://git-scm.com/docs/git-push).\n",
    "3. The commands `wget` and `curl` are useful for grabbing data and files from remote resources off the web.\n",
    "  Read the documentation on each of these commands by typing `man wget` or `man curl` in the terminal.\n",
    "  Make sure you pipe the output to a file or use the proper flags to do so.\n",
    "\n",
    "&#167;  **THERE IS NOTHING TO TURN IN FOR THIS PART.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (20%) Listen to the O'Reilly Data Show Podcast from July 18, 2019: Acquiring and shairing high-quality data with Rogen Chen \n",
    "\n",
    "Data is one of the most important components of modern business and, of course, data manifests\n",
    "in many forms, and involves producers, consumers and value-added uses of data.  Recently,\n",
    "data has become the source of contention, especially as large companies continue to take control\n",
    "of individual's data and use it in ways that do no benefit, and in some cases\n",
    "are used in ways that were neither approved or known by, the the individual directly.\n",
    "\n",
    "There are a number of interesting movements in data protection, such as the European Union's GDPR\n",
    "(General Data Protection Regulation - [https://eugdpr.org](https://eugdpr.org)) that are trying to\n",
    "hold companies account for how individual's data are collected, accessed, shared, distrubuted and protected.\n",
    "Some argue these efforts don't go far enough to acknowledge the true value of data and don't provide\n",
    "access to controlling data in ways that would benefit the individual directly, should they\n",
    "want to have it knowingly used and offered as a product to companies.  In scenarios like these,\n",
    "the individual is then brought into the data transaction as a producer, and companies act as\n",
    "consumers that add value to other consumers, etc.\n",
    "\n",
    "Blockchain has been proposed as a technology that might provide a way to coordinate the\n",
    "development of decentralized and distrubuted data \"ledgers\" that may provide a more complete\n",
    "platform to share data, trace it's lineage, but also validate and facilitate data transactions\n",
    "between consumers and producers.\n",
    "\n",
    "In this podcast, [Ben Lorica](https://www.oreilly.com/talent/4e7ad-ben-lorica) interviews Roger\n",
    "Chen, CEO of Computable Labs, about his recent paper _\"Fair value and decentralized governance of data\"_.\n",
    "Though it is not necessary, you are encouraged to read the very accessible white paper Chen, et al\n",
    "published in June which forms the basis of parts of the conversation in the podcast\n",
    "(that paper is on Github here: [https://git.io/fjpTv]([https://git.io/fjpTv]())).\n",
    "\n",
    "Please listen to the Podcast released July 18, 2019 _\"Acquiring and Sharing High-Quality Data\"_ on\n",
    "the O'Reilly Data Show.  You can listen to it from one of the two links below:\n",
    "\n",
    "* [https://soundcloud.com/oreilly-radar/acquiring-and-sharing-high-quality-data](https://soundcloud.com/oreilly-radar/acquiring-and-sharing-high-quality-data)\n",
    "\n",
    "* [https://www.oreilly.com/ideas/acquiring-and-sharing-high-quality-data](https://www.oreilly.com/ideas/acquiring-and-sharing-high-quality-data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#167;  **PLEASE ANSWER THE FOLLOWING 8 QUESTIONS AFTER LISTENING TO THE PODCAST:**\n",
    "\n",
    "1. Early in the interview Roger was asked what problems were being approached (and attempted to being solved) through the white paper \"Fair value and decentralized governance of data\". What two core areas of data did Roger say he and his Computable Labs company trying to solve?\n",
    "\n",
    "2. In the Enterprise use case, what did Roger say about how to determine who owns data?\n",
    "\n",
    "3. What did Roger say are some of the problems with company's centralizing data as it stands right now?\n",
    "\n",
    "4. Roger describes a simple example of the basic workflow of how data is served by his technology\n",
    "through the use of decentralized \"protocol contracts\" and what he calls the \"Datatrust\" storage.\n",
    "In your own words, describe how this process works.\n",
    "\n",
    "5. Roger suggests a data marketplace model of service providers so that there is decentralized\n",
    "universal access to data, and mentions how \"next generation crowd-source models\"  will propel\n",
    " the fair market value of data. What property did he say\n",
    "this fair market value would be based on to derive this value?\n",
    "\n",
    "6. Roger is bullish on Data privacy and encryption technologies and how we might have a future where\n",
    "data on the Internet is open, fully encrypted and secure \"in the open\".  What two companies did\n",
    "he think would be one's to watch in this space?\n",
    "\n",
    "7. What is a \"synthetic\" dataset (as described in the interview), and what problems did Ben challenge\n",
    "Roger on with a market where these datasets might exist?\n",
    "\n",
    "8. Which Blockchain network will the project be available for testing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (40%) Explore Python for basic text mining \n",
    "\n",
    "Python is one of the most important languages in contemporary data science right now.  Its strengths are in\n",
    "readability and clarity of computational expressiveness.  There are a number of data science libraries and\n",
    "modules that we will be using throughout the course to achieve many important outcomes with Python.\n",
    "\n",
    "You will be tasked with writing two basic programs in Python using Jupyter Lab.  For starters,\n",
    "here are a few recommended resources you are encouraged to use though you are free to use whatever\n",
    "resources you find useful:\n",
    "\n",
    "* [Python website (https://python.org)](https://python.org)\n",
    "* [Hitchhikers Guide to Python by Kenneth Reitz and Tanya Schlusser (https://docs.python-guide.org)](https://docs.python-guide.org)\n",
    "* [Think Python by Allen Downey (http://greenteapress.com/wp/think-python/)](http://greenteapress.com/wp/think-python/)\n",
    "\n",
    "For the first part of the assignment, you will be tasked with a \"warm up\" tapping into the text processing\n",
    "capabilities of Python.  **Please turn in the code and answers according to the instructions.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#167;  **WRITE A PROGRAM IN YOUR JUPYTER NOTEBOOK TO KEEP TRACK OF THE FREQUENCY OF THE TOP 30 CAPITALIZED WORDS IN\n",
    " THE FOLLOWING 5 BOOKS ON GUTENBERG.ORG:**\n",
    "\n",
    "(_The top 30 should be sorted by descending frequency -- most frequent to least._)\n",
    "\n",
    "* The Republic by Plato [http://www.gutenberg.org/cache/epub/1497/pg1497.txt](http://www.gutenberg.org/cache/epub/1497/pg1497.txt)\n",
    "* Don Quixote by Miguel de Cervantes Saavedra [http://www.gutenberg.org/cache/epub/996/pg996.txt](http://www.gutenberg.org/cache/epub/996/pg996.txt)\n",
    "* Dracula by Bram Stoker [http://www.gutenberg.org/cache/epub/345/pg345.txt](http://www.gutenberg.org/cache/epub/345/pg345.txt)\n",
    "* Siddhartha by Herman Hesse [http://www.gutenberg.org/cache/epub/2500/pg2500.txt](http://www.gutenberg.org/cache/epub/2500/pg2500.txt)\n",
    "* Wonderful Wizard of Oz by L. Frank Baum [http://www.gutenberg.org/cache/epub/55/pg55.txt](!wget http://www.gutenberg.org/cache/epub/55/pg55.txt)\n",
    "\n",
    "\n",
    "For this part we will be using the open and freely available \n",
    "[Gutenberg.org](https://Gutenberg.org), which contains a large number of \n",
    "books in the public domain available for reading (or whatever purpose).\n",
    "We will just grab the text documents we would like to analyze and process them, \n",
    "though you are welcome to hang out on Gutenberg and do some light evening \n",
    "reading whenever you have free time.  \n",
    "\n",
    "To speed things up, you can store the documents\n",
    "locally (grabbing them through `wget` or `curl`) &mdash; you can use the\n",
    "boilerplate code provided below that\n",
    "uses the Jupyter [\"system magic\"](https://ipython.readthedocs.io/en/stable/interactive/magics.html#magic-system) commands.  Thus,\n",
    "```bash\n",
    "!wget http://www.gutenberg.org/cache/epub/55/pg55.txt\n",
    "```\n",
    "\n",
    "will retrieve the file and store it in the local directory of the running notebook.  **NOTE:** running such\n",
    "commands could be problematic for notebook cross compatibility for Windows machines which may not\n",
    "have the `wget` command installed.  Use this idiom with caution as you move forward!\n",
    "\n",
    "You will find it useful to read the documentation\n",
    "for the [`Counter`](https://docs.python.org/3.6/library/collections.html?highlight=counter#collections.Counter) object that is part of the\n",
    "Python `collections` module.  While `Counter` may be useful, it is not required\n",
    "and there is a solution without it that is equally elegant.  NOTE:\n",
    "you should **normalize the text** so that all words are lowercase (after the capitals have been extracted)\n",
    "and free of punctuation (commas, periods, dashes, semicolons, etc.).\n",
    "\n",
    "**DO NOT use any other text processing libraries beyond those\n",
    "provided within Python directly (the good news is that you won't\n",
    "have to, but ask me if you have any doubts).**  You also do not\n",
    "need to do any stripping of text from the preamble, introductory\n",
    "remarks, etc. from the documents. Process them as they are.\n",
    "\n",
    "You may  find that [`split()`](https://docs.python.org/3.6/library/stdtypes.html#str.split) is useful in\n",
    "accomplishing tasks for this part, and the [`requests`](http://docs.python-requests.org/en/master/)\n",
    "library is useful for grabbing the text directly from Gutenberg.org\n",
    "as shown in the demo code below, though you are free to use whatever\n",
    "HTTP library (e.g. [`urllib`](https://docs.python.org/3.6/library/urllib.html))\n",
    "you like.  Finally, you can consider normalizing text with the\n",
    "[`re`](https://) regular expressions library.  For example, you can\n",
    "remove all `,` (comma) from a word with `re.sub(r',','','thus,')`.  To extend that \n",
    "to all punctuation, read the documentation on character classes and\n",
    "punctuation [https://docs.python.org/2/library/re.html#regular-expression-syntax](https://docs.python.org/2/library/re.html#regular-expression-syntax).\n",
    "\n",
    "Your notebook should include the solution code and answers structured such that you will display\n",
    "the word and the frequency of that word either as a list of tuples or a table.  You are free to do either,\n",
    "but the solution should be in your notebook directly, not in a file.\n",
    "\n",
    "**>** _here is code using_ `requests` _to read in a text file directly into memory from gutenberg.org and\n",
    "store it in a variable called_ `data`.\n",
    "\n",
    "```python\n",
    "import requests\n",
    "\n",
    "url = \"http://www.gutenberg.org/files/31475/31475-0.txt\"\n",
    "r = requests.get(url)\n",
    "\n",
    "if r.status_code == 200:\n",
    "   data = r.content\n",
    "\n",
    "   # YOUR CODE TO PROCESS THIS DOCUMENT\n",
    "\n",
    "else:\n",
    "   print(\"[warn] GET request did not return HTTP/200 (HTTP/{} returned instead\".format(r.status_code))\n",
    "```\n",
    "\n",
    "You are advised to use a function that you can reuse and call again over all 5 files, though if you do not\n",
    "use a function, you will not lose points if your answer is correct without it.  A Python class is not\n",
    "necessary for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import string\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_url(url):\n",
    "    # function to process the text from the URL\n",
    "    r = requests.get(url)\n",
    "\n",
    "    if r.status_code == 200:\n",
    "        data = r.text\n",
    "        \n",
    "        # removed the punctuations in the text\n",
    "        remove_punc = re.sub('['+string.punctuation+']', '',data)\n",
    "        \n",
    "        # removes returns and split the text to an array of words\n",
    "        remove_newline = remove_punc.replace('\\r',' ')\n",
    "        words = remove_newline.split()\n",
    "\n",
    "        # adds the word in the array of capitalized words\n",
    "        for w in words:\n",
    "            if w[0].isupper():\n",
    "                cap_words.append(w)\n",
    "\n",
    "    else:\n",
    "        print(\"[warn] GET request did not return HTTP/200 (HTTP/{} returned instead\".format(r.status_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81838\n"
     ]
    }
   ],
   "source": [
    "urls = [\"http://www.gutenberg.org/cache/epub/1497/pg1497.txt\", \"http://www.gutenberg.org/cache/epub/996/pg996.txt\",\n",
    "       \"http://www.gutenberg.org/cache/epub/345/pg345.txt\", \"http://www.gutenberg.org/cache/epub/2500/pg2500.txt\",\n",
    "       \"http://www.gutenberg.org/cache/epub/55/pg55.txt\"]\n",
    "\n",
    "cap_words = []\n",
    "\n",
    "for link in urls:\n",
    "    read_url(link)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 13835),\n",
       " ('The', 2667),\n",
       " ('Don', 2638),\n",
       " ('Sancho', 2056),\n",
       " ('Quixote', 2053),\n",
       " ('And', 1847),\n",
       " ('He', 1389),\n",
       " ('But', 1266),\n",
       " ('It', 873),\n",
       " ('God', 714),\n",
       " ('Then', 672),\n",
       " ('In', 573),\n",
       " ('There', 563),\n",
       " ('You', 555),\n",
       " ('THE', 545),\n",
       " ('State', 532),\n",
       " ('We', 529),\n",
       " ('They', 529),\n",
       " ('What', 528),\n",
       " ('Yes', 525),\n",
       " ('That', 499),\n",
       " ('If', 483),\n",
       " ('This', 482),\n",
       " ('When', 474),\n",
       " ('For', 461),\n",
       " ('OF', 454),\n",
       " ('Project', 415),\n",
       " ('To', 402),\n",
       " ('Siddhartha', 376),\n",
       " ('Dorothy', 346)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = collections.Counter(cap_words)\n",
    "\n",
    "# shows the Top 30 most frequent capitalized words in the 5 files provided\n",
    "c.most_common(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (40%) Explore Python for data munging and analysis, with an introduction to JSON and Pandas \n",
    "\n",
    "\n",
    "Python's strengths shine when tasked with data munging and analysis. As you may already know,\n",
    "many cities are beginning the process of prototyping \"Smart City\" designs, which in this\n",
    "first generation of such technologies, involve outfitting cities with sensors which capture\n",
    "a variety of data, from basic _meteorological measurements_ (temperature, humidity, barometric pressure)\n",
    "to _air quality measurements_ (such as VOC &mdash; Volatile Organic Compounds), to _audio and image/video_\n",
    "data.  Be confident that such efforts are going to expand, and that data scientists\n",
    "such as yourself will be called upon to help analyse and bring value to this data for decision makers\n",
    "and citizens alike.\n",
    "\n",
    "In this part of the homework, we will be working with a real-time datastream (or a recent slice of it)\n",
    "that is part of the City of Chicago's [\"Array of Things\" (AOT)](https://arrayofthings.github.io) project.  The\n",
    "sensor arrays being developed\n",
    "and deployed (now also being piloted in other cities such as London, Austin and San Francisco) stream\n",
    "real-time public data and are giving us a sense of the kinds of questions that will need to be\n",
    "answered about urban landscapes of the future &mdash; at this point, there are far more questions\n",
    "than answers!\n",
    "\n",
    "These data are available via APIs which emit JSON datasets, making them easy to manipulate via a variety of\n",
    "easily available tools in nearly every modern programming language and data science toolkit.\n",
    "\n",
    "**Please familiarize yourself with the Array of Things project here**:\n",
    "\n",
    "* [Array of Things (AOT)](https://arrayofthings.github.io) main project website\n",
    "* [Array of Things](https://medium.com/array-of-things) blog\n",
    "\n",
    "**Furthermore familiarize yourself with the data access and HTTP API documentation here**:\n",
    "\n",
    "* [Array of Things Releases APIs for Chicago Data, Enabling Applications](https://medium.com/array-of-things/array-of-things-releases-apis-for-chicago-data-enabling-applications-9bfdbe477df3): Medium post / Nov. 5, 2018\n",
    "* [Array of Things HTTP API](https://arrayofthings.docs.apiary.io/#reference) for data access\n",
    "\n",
    "\n",
    "As you will notice the data is being emitted as JSON data.  If you are unfamiliar with JSON, please\n",
    "start at this resource: [the JSON specification](https://json.org).\n",
    "\n",
    "In this part of the assignment, we will make use of Python libraries to pull the data from the API\n",
    "endpoint and use [Pandas](https://pandas.pydata.org) to plot the data.  You will notice that the API\n",
    "allows the user to filter many aspects of the data, from instrument type, to the number of measurements\n",
    "returned per payload.  We will focus on a very specific subset of data to get our feet wet at first, but\n",
    "will expand our useage later.\n",
    "\n",
    "We will focus on two data points PM2.5 and PM10, which correspond to particulate matter and pollution\n",
    "in the air that are 2.5&#x00B5; (microns) and 10&#x00B5; respectively.  Typically, this PM measurement\n",
    "is an indicator of air quality and as the values increase, the air quality decreases correspondingly.\n",
    "Typical values\n",
    "under 100 are considered acceptable for human health, while over 100, respiratory distress may occur, especially\n",
    "in vulenrable populations such as children, the elderly and those with lung diseases such as asthma.  PM\n",
    "is only one measurement of air quality and is typically combined with other measurements to provide\n",
    "a more comprehensive view of air quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#167;  **WRITE THE CODE IN YOUR NOTEBOOK TO LOAD THE JSON DATA FOR THE AIR QUALITY MONITORS FOR THE WEEK OF AUGUST 25, 2019.**:\n",
    "\n",
    "_You will need to perform the following steps_:\n",
    "\n",
    "1. **make an API call to load data**; use the `requests` library method `get(url)` to do this\n",
    "2. **filter the data for only the air quality monitors for PM2.5 and PM10 VOC detection**;\n",
    "   you will need to make 2 calls and use the API parameter `sensor=plantower.pms7003.pm10_atm` for\n",
    "   one and `sensor=plantower.pms7003.pm25_atm`\n",
    "3. **restrict the dataset to the sensor node** `0C1`; use the API parameter `node=0C1`\n",
    "4. **iterate over the calls 40 times, each time pulling 260 points of data**;\n",
    "   use the API parameter `size` and a loop (or function wrapped in a loop) for 40 iterations,\n",
    "5. **store the entire JSON data locally in a two files** and name them `0c1.pm10.2019-08-31.json` and\n",
    "`0c1.pm25.2019-08-31.json`.\n",
    "\n",
    "\n",
    "**HINT:** _Here is a code snippet you might like to study and use to craft a solution:_\n",
    "\n",
    "```python\n",
    "AOT_ENDPOINT = \"https://api-of-things.plenar.io/api/observations\"\n",
    "r = requests \\\n",
    "      .get(\"{}?&timestamp=gt:2019-08-25T00:00\" \\\n",
    "      .format(AOT_ENDPOINT)\n",
    "\n",
    "if r.status_code == 200:\n",
    "  payload = r.json()\n",
    "\n",
    "  # get the next link to make the next call\n",
    "  next = payload['meta']['links']['next']\n",
    "  data = payload['data']\n",
    "\n",
    "  # YOUR CODE TO ITERATE OVER THE DATA\n",
    "\n",
    "else:\n",
    "   print(\"[warn] GET request did not return HTTP/200 (HTTP/{} returned instead\".format(r.status_code))\n",
    "\n",
    "```\n",
    "\n",
    "Notice, we stored the output as a JSON object (`data = r.json()`).\n",
    "You will need to familiarize yourself with the\n",
    "[`json`](https://docs.python.org/3.6/library/json.html) library and\n",
    "the `json` object.  Specifically, how JSON is transformed into a\n",
    "Python dictionary allowing easy manipulation.  Look at an example usage\n",
    "of the `json` library: [https://docs.python-guide.org/scenarios/json/](https://docs.python-guide.org/scenarios/json/).\n",
    "There are also many other examples you can explore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "AOT_ENDPOINT = \"https://api-of-things.plenar.io/api/observations\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_writer(sensor, filename):\n",
    "    r = requests \\\n",
    "    .get(\"{}?order=asc:timestamp&size=260&node=0C1&sensor={}&timestamp=gt:2019-08-25T00:00:00\" \\\n",
    "    .format(AOT_ENDPOINT, sensor))\n",
    "\n",
    "    if r.status_code == 200:\n",
    "\n",
    "        payload = r.json()\n",
    "\n",
    "        next = payload['meta']['links']['next']\n",
    "        data = payload['data']\n",
    "\n",
    "        with open(filename, \"w\") as outfile:\n",
    "            json.dump(data, outfile)\n",
    "\n",
    "        for i in range(39):\n",
    "            new_r = requests.get(next)\n",
    "            new_payload = new_r.json()\n",
    "\n",
    "            next = new_payload['meta']['links']['next']\n",
    "            new_data = new_payload['data']\n",
    "\n",
    "            with open(filename) as outfile:\n",
    "                old_data = json.load(outfile)\n",
    "                data = old_data + new_data\n",
    "\n",
    "            with open(filename, \"w\") as outfile:\n",
    "                json.dump(data, outfile)\n",
    "\n",
    "    else:\n",
    "        print(\"[warn] GET request did not return HTTP/200 (HTTP/{} returned instead\".format(r.status_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_writer(\"plantower.pms7003.pm10_atm\", \"0c1.pm10.2019-08-31.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_writer(\"plantower.pms7003.pm25_atm\", \"0c1.pm25.2019-08-31.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#167;  **USE PANDAS TO LOAD THE JSON DATA TO A DATAFRAME AND ANSWER THE FOLLOWING QUESTIONS:**\n",
    "\n",
    "1. What are the mean PM2.5 and PM10 values from each dataset?\n",
    "3. When is the smalled PM2.5 value recorded (date, including minutes and seconds are\n",
    "   necessary for full points)?\n",
    "4. How many times did the PM10 exceed 16.0 and on what day(s)?\n",
    "5. What was the average PM2.5 on 2019-08-26?\n",
    "\n",
    "To answer these questions, you'll need to dive further into Pandas, which is\n",
    "the standard tool in the Python data science stack for loading, manipulating,\n",
    "transforming, analyzing and preparing data as input to other tools such as\n",
    "[Numpy (http://www.numpy.org/)](http://www.numpy.org/), \n",
    "[SciKitLearn (http://scikit-learn.org/stable/index.html)](http://scikit-learn.org/stable/index.html), \n",
    "[NLTK (http://www.nltk.org/)](http://www.nltk.org/) and others.\n",
    "\n",
    "For this assignment, you will only need to learn how to load and select data using Pandas.\n",
    "\n",
    "**LOADING DATA**\n",
    "The core data structure in Pandas is the `DataFrame`.  You will need to visit\n",
    "the Pandas documentation [(http://pandas.pydata.org/pandas-docs)](http://pandas.pydata.org/pandas-docs)\n",
    "to learn more about the library, but to help you along with a hint, read the\n",
    "documentation on the [`pandas.read_json()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_json.html) method.\n",
    "\n",
    "**SELECTING DATA**\n",
    "The [tutorial here on indexing and selecting](http://pandas.pydata.org/pandas-docs/stable/indexing.html)\n",
    "should be of great use in understanding how to index and select subsets of\n",
    "the data to answer the questions.\n",
    "\n",
    "**EXAMPLE CODE**\n",
    "\n",
    "**>** _Here is example code that should give you clues about the structure\n",
    "of your code for this part._\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "df = pandas.read_json('your_json_file.json')\n",
    "\n",
    "# code for question 1 ... and so on\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "      <th>node_vsn</th>\n",
       "      <th>sensor_path</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>uom</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'type': 'Feature', 'geometry': {'type': 'Poin...</td>\n",
       "      <td>0C1</td>\n",
       "      <td>plantower.pms7003.pm10_atm</td>\n",
       "      <td>2019-09-03 00:00:07</td>\n",
       "      <td>μg/m^3</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'type': 'Feature', 'geometry': {'type': 'Poin...</td>\n",
       "      <td>0C1</td>\n",
       "      <td>plantower.pms7003.pm10_atm</td>\n",
       "      <td>2019-09-03 00:00:34</td>\n",
       "      <td>μg/m^3</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'type': 'Feature', 'geometry': {'type': 'Poin...</td>\n",
       "      <td>0C1</td>\n",
       "      <td>plantower.pms7003.pm10_atm</td>\n",
       "      <td>2019-09-03 00:01:01</td>\n",
       "      <td>μg/m^3</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'type': 'Feature', 'geometry': {'type': 'Poin...</td>\n",
       "      <td>0C1</td>\n",
       "      <td>plantower.pms7003.pm10_atm</td>\n",
       "      <td>2019-09-03 00:01:27</td>\n",
       "      <td>μg/m^3</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'type': 'Feature', 'geometry': {'type': 'Poin...</td>\n",
       "      <td>0C1</td>\n",
       "      <td>plantower.pms7003.pm10_atm</td>\n",
       "      <td>2019-09-03 00:01:53</td>\n",
       "      <td>μg/m^3</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            location node_vsn  \\\n",
       "0  {'type': 'Feature', 'geometry': {'type': 'Poin...      0C1   \n",
       "1  {'type': 'Feature', 'geometry': {'type': 'Poin...      0C1   \n",
       "2  {'type': 'Feature', 'geometry': {'type': 'Poin...      0C1   \n",
       "3  {'type': 'Feature', 'geometry': {'type': 'Poin...      0C1   \n",
       "4  {'type': 'Feature', 'geometry': {'type': 'Poin...      0C1   \n",
       "\n",
       "                  sensor_path           timestamp     uom  value  \n",
       "0  plantower.pms7003.pm10_atm 2019-09-03 00:00:07  μg/m^3     10  \n",
       "1  plantower.pms7003.pm10_atm 2019-09-03 00:00:34  μg/m^3     11  \n",
       "2  plantower.pms7003.pm10_atm 2019-09-03 00:01:01  μg/m^3     11  \n",
       "3  plantower.pms7003.pm10_atm 2019-09-03 00:01:27  μg/m^3     12  \n",
       "4  plantower.pms7003.pm10_atm 2019-09-03 00:01:53  μg/m^3     12  "
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_10 = pd.read_json('0c1.pm10.2019-08-31.json')\n",
    "df_10.head()\n",
    "# len(df_10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "      <th>node_vsn</th>\n",
       "      <th>sensor_path</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>uom</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'type': 'Feature', 'geometry': {'type': 'Poin...</td>\n",
       "      <td>0C1</td>\n",
       "      <td>plantower.pms7003.pm25_atm</td>\n",
       "      <td>2019-09-03 00:00:07</td>\n",
       "      <td>μg/m^3</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'type': 'Feature', 'geometry': {'type': 'Poin...</td>\n",
       "      <td>0C1</td>\n",
       "      <td>plantower.pms7003.pm25_atm</td>\n",
       "      <td>2019-09-03 00:00:34</td>\n",
       "      <td>μg/m^3</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'type': 'Feature', 'geometry': {'type': 'Poin...</td>\n",
       "      <td>0C1</td>\n",
       "      <td>plantower.pms7003.pm25_atm</td>\n",
       "      <td>2019-09-03 00:01:01</td>\n",
       "      <td>μg/m^3</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'type': 'Feature', 'geometry': {'type': 'Poin...</td>\n",
       "      <td>0C1</td>\n",
       "      <td>plantower.pms7003.pm25_atm</td>\n",
       "      <td>2019-09-03 00:01:27</td>\n",
       "      <td>μg/m^3</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'type': 'Feature', 'geometry': {'type': 'Poin...</td>\n",
       "      <td>0C1</td>\n",
       "      <td>plantower.pms7003.pm25_atm</td>\n",
       "      <td>2019-09-03 00:01:53</td>\n",
       "      <td>μg/m^3</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            location node_vsn  \\\n",
       "0  {'type': 'Feature', 'geometry': {'type': 'Poin...      0C1   \n",
       "1  {'type': 'Feature', 'geometry': {'type': 'Poin...      0C1   \n",
       "2  {'type': 'Feature', 'geometry': {'type': 'Poin...      0C1   \n",
       "3  {'type': 'Feature', 'geometry': {'type': 'Poin...      0C1   \n",
       "4  {'type': 'Feature', 'geometry': {'type': 'Poin...      0C1   \n",
       "\n",
       "                  sensor_path           timestamp     uom  value  \n",
       "0  plantower.pms7003.pm25_atm 2019-09-03 00:00:07  μg/m^3     10  \n",
       "1  plantower.pms7003.pm25_atm 2019-09-03 00:00:34  μg/m^3     11  \n",
       "2  plantower.pms7003.pm25_atm 2019-09-03 00:01:01  μg/m^3     11  \n",
       "3  plantower.pms7003.pm25_atm 2019-09-03 00:01:27  μg/m^3     12  \n",
       "4  plantower.pms7003.pm25_atm 2019-09-03 00:01:53  μg/m^3     12  "
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_25 = pd.read_json('0c1.pm25.2019-08-31.json')\n",
    "df_25.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.352949226768851\n"
     ]
    }
   ],
   "source": [
    "print(df_10['value'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#167;  **BUILD A GRAPH OF EACH PM DATA SET (WITH A PANDAS DATAFRAME)**\n",
    "\n",
    "Your solution should display the\n",
    "data on a single graph by using the `DataFrame.plot()` method. You can study the Python snippet\n",
    "below for a hint:\n",
    "\n",
    "```python\n",
    "...\n",
    "\n",
    "ax = pm25.plot(figsize=(30,10))\n",
    "pm10.plot(ax=ax)\n",
    "\n",
    "...\n",
    "```\n",
    "\n",
    "where `pm25` and `pm10` are the Dataframes containing the JSON data extracted from the files produced\n",
    "in your prior code (see Figure 1 below for a sample graph with random data).\n",
    "\n",
    "![Sample graph with two data sets](sample_graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#167;  **CREATE A PM2.5 NOWCAST FOR THE 12 HOUR TIME PERIOD 2019-08-27T00:00 TO 2019-08-27T12:00.**\n",
    "\n",
    "[Nowcasts](https://en.wikipedia.org/wiki/NowCast_(air_quality_index)) are simple\n",
    "tools that help assess a quantity (usually some air quality measurement)\n",
    "for determining the present state of that quantity, while taking into account data from\n",
    "the recent past.  For example, to understand how air quality impacts us right now, we should\n",
    "also consider what the air quality has been recently, since the current measurement may not\n",
    "tell the full story of the changes over a recent window of time.\n",
    "\n",
    "The Nowcast for _air quality_ is often computed by the following formula:\n",
    "\n",
    "$$ I(C_{h=12}, w) = \\frac{\\sum^h_{i=1} c_i \\times  w^{i-1}} {\\sum^h_{i=1} w^{i-1}} $$\n",
    "\n",
    "where $c_{i}$ is the PM (in our case PM2.5) concentration for a given hour $h$ and $w^i$ the\n",
    "exponentiated weight calculation assigned for hour $i$.  Consider $h=12$,\n",
    "representing a 12 hour window of time.  For every hour $h = 1 \\ldots 12$, $c_1$ is the PM2.5 concentration\n",
    "for hour 1, $c_2$ is the PM2.5 concentration for hour 2 and so on to $c_h$.  The hour concentrations\n",
    "are computed from\n",
    "most recent to least, thus, $c_1$ is the most recent hour (e.g. one hour ago), and $c_{12}$ is 12 hours prior.\n",
    "\n",
    "Let $C_{h=12} = {c_1, c_2, \\ldots, c_{12}}$ and $c_{max} = \\text{max}\\{{C_h}\\}$ and $c_{min} = \\text{min}\\{{C_h}\\}$.\n",
    "\n",
    "Also, let\n",
    "$$w^* = \\frac{c_{min}}{c_{max}}.$$\n",
    "\n",
    "Then the weights $w$ above are computed as such,\n",
    "$$\n",
    " w =\n",
    "  \\begin{cases}\n",
    "    w^*  & \\quad \\text{if } w^* > 0.5 \\\\\n",
    "    0.5  & \\quad \\text{if } w^* \\le 0.5\n",
    "  \\end{cases}\n",
    "$$\n",
    "\n",
    "A simple example follows:\n",
    "\n",
    "$$C_{12} = 60, 60, 60, 60, 70, 70, 70, 70, 80, 80, 80, 80$$\n",
    "\n",
    "Assume $c_1 = 60, c_4 = 70$ and $c_12 = 80$.\n",
    "Then,\n",
    "$$w^{*} = \\frac{60}{80} = 0.75$$\n",
    "\n",
    "So, $w = 0.75$.\n",
    "\n",
    "Now,\n",
    "$$I(C_{12}, w=0.75) = \\frac{60\\times0.75^0 + 60\\times0.75^1 + \\cdots + 80\\times0.75^{11}} {0.75^0 + 0.75^1 + \\cdots + 0.75^{11}}$$\n",
    "\n",
    "Thus,\n",
    "\n",
    "$$I(C_{12}, w=0.75) = 63.6471$$\n",
    "\n",
    "You will need to write a Python function `compute_nowcast()` which will take as input the\n",
    "Dataframe which has the hourly PM2.5 average of the 12-hour window, and returns the\n",
    "single nowcast value $I$.\n",
    "\n",
    "Here are some hints:\n",
    "\n",
    "* The following code will take a Dataframe with a column of Datetimes and convert it into a timeseries\n",
    "indexed Dataframe that can be grouped over each hour.  Assmume `df` is 12 hour window\n",
    "of data that has a column `dt` which\n",
    "is a date string of the format similar to: `2019-08-27T10:59:13`.  HINT: your data coming from the API\n",
    "looks similar to this.  Now following the code:\n",
    "\n",
    "```python\n",
    "df.dt = pd.to_datetime(df.dt) # convert the dt column to datetime\n",
    "\n",
    "# make the dt column the Dataframe index\n",
    "df.set_index(df.dt, inplace=True)\n",
    "\n",
    "# now that the index is dt, we don't need the column\n",
    "df = df.drop(columns=['dt'])\n",
    "\n",
    "# this will return a 12 element Dataframe with datetime index averaged over every hour\n",
    "day_12h_window = df.groupby(df.index.hour).mean()[:12]\n",
    "```\n",
    "will produce a 12-value Dataframe that you can then run your `compute_nowcast()` over, once implemented."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": "1",
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
